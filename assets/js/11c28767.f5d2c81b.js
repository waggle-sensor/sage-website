"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5320],{9461:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/super_resolution_results2-3fbdeec19f6dc3dcfe16bb38f0baec22.png"},15429:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/super_resolution_supir_arch-630d7fc7588033e8763d2d79f00ab770.png"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(96540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}},42110:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/super_resolution_results1-7766d45f8f9ea8485a28970a0ff8454e.png"},47749:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/super_resolution_gan_model-27de835e5e62e94e2dab63911353b4d2.png"},54260:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"recent/super-resolution","title":"Exploration of Super Resolution Image Enhancement","description":"Introduction","source":"@site/science/recent/super-resolution.md","sourceDirName":"recent","slug":"/recent/super-resolution","permalink":"/science/recent/super-resolution","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Super Resolution Image Enhancement","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Sound Separation","permalink":"/science/recent/sound-separation"},"next":{"title":"LIDAR for Solar Estimation and Sky Classification","permalink":"/science/recent/lidar-sky-solar"}}');var s=i(74848),o=i(28453);const r={sidebar_label:"Super Resolution Image Enhancement",sidebar_position:3},a="Exploration of Super Resolution Image Enhancement",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Methods",id:"methods",level:2},{value:"Results",id:"results",level:2},{value:"Summary",id:"summary",level:2},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"exploration-of-super-resolution-image-enhancement",children:"Exploration of Super Resolution Image Enhancement"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Super Resolution is an Image Enhancement Technique, where the goal is to take a low-resolution image and increase its resolution to improve its quality. In contrast with the reverse operation called the downscaling task, where a large area of pixels is converted into a small area of pixels, Super Resolution is a difficult task to execute. There are limited pixels on the original image to work with and the model must then predict and generate artificial data to create an improved image."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"super_resolution",src:i(90964).A+"",width:"622",height:"333"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://towardsdatascience.com/image-super-resolution-an-overview-of-the-current-state-of-research-94294a77ed5a",children:'Credit to Christian Galea\u2019s Towards Data Science article \u201cImage Super-Resolution: An Overview of the Current State of Research"'})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"This project serves to explore different types of super-resolution models and understand the feasibility of deploying such models into laboratory systems."})}),"\n",(0,s.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,s.jsx)(n.p,{children:"Super Resolution plays a critical role in various fields restoring old images to learn more about the history of our world, analyzing satellite images to create improved maps, and analyzing microscopic organisms in medical diagnoses. As technological and scientific advancements continue to improve, there is an increase in demand for image enhancement tools: the aid of AI in enhancing images helps play a pivotal role in decreasing the costs of developing advanced microscopes, telescopes, and cameras."}),"\n",(0,s.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,s.jsx)(n.p,{children:"For this project, I explored two different types of models that have played a revolutionary role in the Super Resolution field: GAN (Generative Advertisal Network) models and Diffusion models. I specifically chose the Real-ESRGAN model and Diffusion model because they were the best models for their respective models from my research. Here is a brief list of facts about each model."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GAN Model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A Generator and a Discriminator compete against each other to mutually boost performance"}),"\n",(0,s.jsx)(n.li,{children:"The Generator creates a fake image using noise (all at once) to fool the Discriminator"}),"\n",(0,s.jsx)(n.li,{children:"The Discriminator tries to guess the real image between the generated image and the actual image"}),"\n",(0,s.jsx)(n.li,{children:"May not produce the best results if the Discriminator or the Generator is not effective enough"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-ESRGAN:"})," has a powerful Discriminator contrary to other GAN models"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"gan_model",src:i(47749).A+"",width:"687",height:"342"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://developer.ibm.com/articles/generative-adversarial-networks-explained/",children:'Credit to Caper Hansen\u2019s IBM Developer article \u201cGenerative adversarial networks explained"'})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Diffusion Model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A method that outperforms GAN models in rendering quality"}),"\n",(0,s.jsx)(n.li,{children:"A type of Generator"}),"\n",(0,s.jsxs)(n.li,{children:["Adds and removes noise from the original image at a slower rate","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Better method than the GAN Model generator since it gives the model additional time to learn complex patterns"}),"\n",(0,s.jsx)(n.li,{children:"Main drawback is the time complexity will be longer"}),"\n",(0,s.jsxs)(n.li,{children:["Process is similar to a thermodynamics problem","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Analogy for adding noise:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adding a drop of food coloring into a large bowl of water has a very high probability of the food coloring affecting the other water droplets"}),"\n",(0,s.jsx)(n.li,{children:"Results in all the water droplets being affected by food coloring"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Analogy for removing noise:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Slowly rewinding time and pinpointing the exact location of where the initial drop of food coloring came from starting from when the bowl is filled with food coloring water droplets"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Adjusts loss to make sure the image is as high quality as possible"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"diffusion_model",src:i(96822).A+"",width:"687",height:"230"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://medium.com/@cch57/exploring-the-promise-of-generative-models-in-chemistry-an-introduction-to-diffusion-models-31530e9d1dcb",children:'Credit to Charlie Harris\u2019 Medium Article \u201cDiffusion Models in Generative Chemistry for Drug Design"'})}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SUPIR:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["There is an attached text prompt option to help guide the model to the correct output","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Can plug in a Multi-Modal Large Language Model"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Extremely computationally expensive, and the model is very large"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"supir_architecture",src:i(15429).A+"",width:"1440",height:"386"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2401.13627",children:"Credit to XPixel Group\u2019s Supir Research Paper"})}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.em,{children:"Encoder Decoder: Input and Output Source"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.em,{children:"SDXL: StableDiffusion Model"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.em,{children:"ControlNet: Controls Diffusion Models"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.em,{children:"Multi-Modal Model: Outputs text describing a given image"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,s.jsx)(n.p,{children:"Below are tables highlighting the results when using each of the models. The Numbers in the Input Image column highlight the dimensions of each image. The input images are less than 250x250 pixels in dimension and the output images are upscaled at least 4x in size to highlight the differences in generation."}),"\n",(0,s.jsx)(n.p,{children:"For the SUPIR model, a prompt was inserted into the Multi-Modal Large Language Model to guide the model to the correct output as mentioned before. Below were the prompts for each of the tables:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Trees and Bushes: A snowy landscape with bushes and trees nearby"}),"\n",(0,s.jsx)(n.li,{children:"Shed behind Tree: Red shed with tree in front on a snowy island and ocean scenery in back"}),"\n",(0,s.jsx)(n.li,{children:"Bus Stop: A road surrounded by grass leading to a bus stop."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"From the results below, it is very apparent that SUPIR produces much clearer results than Real-ESRGAN. For both images in the chart below, the trees are rendered very clearly in the SUPIR model, whereas it is more blurry in the Real-ESRGAN model. These differences can also be viewed in the background, which is noticeable in the snow, the dirt, and the ocean that were generated by both the models."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"results_1",src:i(42110).A+"",width:"1045",height:"368"})}),"\n",(0,s.jsx)(n.p,{children:"The following chart provides the finishing blow for proving the effectiveness of the SUPIR model over the Real-ESRGAN model. Although the Real-ESRGAN model was unable to produce a realistic-looking image, the details that were provided in the SUPIR model are very apparent, as shown in the generated quality for the mountainous background, the grassy terrain, the bus stop, the rocky road, the street lights, and the bus sign. The two generated images are as clear as light and day in terms of quality."}),"\n",(0,s.jsx)(n.p,{children:"It is however important to note the drawbacks of the SUPIR model. The same quality image is unable to be generated if a prompt is not used for the SUPIR model: a false prediction was made where the model created a hallucination and generated a plane instead of a bus stop roof. Additionally, the model is poor at generating text: there is incomprehensible text written on the plane. It is also important to highlight the amount of time it took for each model to generate each image. Because SUPIR's model is very large, it takes at least 70 seconds to generate an image compared to Real-ESRGAN's 10 seconds, which is a substantial difference in time."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"results_2",src:i(9461).A+"",width:"1022",height:"664"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Here is a brief list of facts summarizing the advantages and disadvantages of the two models."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Advantages of Real-ESRGAN:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Small model (<1 GB)","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requires <1 GB of VRAM"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Runs on CPU as well"}),"\n",(0,s.jsx)(n.li,{children:"Fast compute time"}),"\n",(0,s.jsx)(n.li,{children:"Deployable on Edge"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Disadvantages of Real-ESRGAN"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Poor results if the image quality is too small"}),"\n",(0,s.jsx)(n.li,{children:"Image enhancement is not very apparent"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Advantages of SUPIR:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Image enhancement is much more powerful compared to Real-ESRGAN"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Disadvantages of SUPIR:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Extremely large model (30-40 GB)","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requires 4~10 GB of VRAM"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Requires GPU"}),"\n",(0,s.jsx)(n.li,{children:"Inefficient for large batch sizes"}),"\n",(0,s.jsx)(n.li,{children:"Poor results if no text is provided"}),"\n",(0,s.jsx)(n.li,{children:"Poor at rendering text"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"This project explored the advantages and disadvantages of Real-ESRGAN and SUPIR and determined that it is difficult to deploy super-resolution models into the laboratory systems since current technology does not provide a proper balance between the number of computational resources, time, and quality generated. More research into the field would need to be performed for Super Resolution to serve as a viable solution to deploy on the edge."}),"\n",(0,s.jsx)(n.p,{children:"It is however important to acknowledge that the SUPIR model displays excellent insight into the potential viability of Super-Resolution, and is very feasible for deployment when a method for decreasing computational power is discovered."})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},90964:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/super_resolution-cd431c47454b3eefec2f56887342d16d.png"},96822:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/super_resolution_diffusion_model-347c354af0db1fa1a4f5d59b6577a247.png"}}]);