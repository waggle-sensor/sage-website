"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2051],{28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(96540);const s={},r=i.createContext(s);function a(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:t},e.children)}},80497:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>h,default:()=>g,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"type":"mdx","permalink":"/labs/image-search","source":"@site/src/pages/labs/image-search.md","title":"Sage Image Search","description":"In the age of AI-powered tools, searching through thousands of images shouldn\'t just be about basic metadata. What if you could search by meaning? That\u2019s exactly what this project aims to do: combine the best of semantic understanding and keyword precision into one powerful hybrid image search engine to help you find relevant image series for your project.","frontMatter":{},"unlisted":false}');var s=n(74848),r=n(28453);const a=n.p+"assets/medias/Image_Search-2248236a060d4481f9589833d9f188dd.mp4",o={},h="Sage Image Search",c={},l=[{value:"What We&#39;re Building",id:"what-were-building",level:2},{value:"The Tech Stack &amp; Pipeline",id:"the-tech-stack--pipeline",level:2},{value:"Step 1: Captioning with Florence 2",id:"step-1-captioning-with-florence-2",level:3},{value:"Step 2: Embedding with ImageBind",id:"step-2-embedding-with-imagebind",level:3},{value:"Step 3: Vector + Keyword Search",id:"step-3-vector--keyword-search",level:3},{value:"Step 4: Hybrid Search Fusion",id:"step-4-hybrid-search-fusion",level:3},{value:"Step 5: Reranking with ms-marco-MiniLM-L6-v2",id:"step-5-reranking-with-ms-marco-minilm-l6-v2",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Why This Matters",id:"why-this-matters",level:2}];function d(e){const t={a:"a",blockquote:"blockquote",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"sage-image-search",children:"Sage Image Search"})}),"\n",(0,s.jsxs)(t.p,{children:["In the age of AI-powered tools, searching through thousands of images shouldn't just be about basic metadata. What if you could search by meaning? That\u2019s exactly what this project aims to do: combine the best of ",(0,s.jsx)(t.strong,{children:"semantic understanding"})," and ",(0,s.jsx)(t.strong,{children:"keyword precision"})," into one powerful ",(0,s.jsx)(t.strong,{children:"hybrid image search engine"})," to help you find relevant image series for your project."]}),"\n",(0,s.jsx)("video",{className:"w-full h-auto",controls:!0,children:(0,s.jsx)("source",{src:a,type:"video/mp4"})}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:"All images in this demo were captured by Sage nodes and retrieved from Beehive in near real-time."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Let\u2019s walk through what it does, how it works, and where it\u2019s going next."}),"\n",(0,s.jsx)(t.h2,{id:"what-were-building",children:"What We're Building"}),"\n",(0,s.jsxs)(t.p,{children:["This project is all about ",(0,s.jsx)(t.strong,{children:"hybrid search"})," \u2014 combining ",(0,s.jsx)(t.strong,{children:"vector-based"})," and ",(0,s.jsx)(t.strong,{children:"keyword-based"})," search \u2014 to enable smarter image retrieval. At its core, the system:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Generates captions"})," for each image using a ",(0,s.jsxs)(t.strong,{children:["LLM (",(0,s.jsx)(t.a,{href:"https://huggingface.co/microsoft/Florence-2-base",children:"Florence 2"}),")"]})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Creates multi-modal embeddings"})," using a ",(0,s.jsxs)(t.strong,{children:["Multimodal Embedding Model (",(0,s.jsx)(t.a,{href:"https://imagebind.metademolab.com",children:"ImageBind"}),")"]})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Stores embeddings"})," using a ",(0,s.jsxs)(t.strong,{children:["Vector Database (",(0,s.jsx)(t.a,{href:"https://weaviate.io",children:"Weaviate"}),")"]})]}),"\n",(0,s.jsxs)(t.li,{children:["Performs ",(0,s.jsx)(t.strong,{children:"vector search"})," (using those embeddings)"]}),"\n",(0,s.jsxs)(t.li,{children:["Also does a traditional ",(0,s.jsx)(t.strong,{children:"keyword search"})," on the image captions and metadata"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Combines the results"})," into a hybrid search result set"]}),"\n",(0,s.jsxs)(t.li,{children:["Finally, applies a ",(0,s.jsxs)(t.strong,{children:["reranker model (",(0,s.jsx)(t.a,{href:"https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2",children:"ms-marco-MiniLM-L6-v2"}),")"]})," to prioritize the most relevant results"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["Why all this? Because combining these techniques helps you go beyond surface-level matching \u2014 it helps find the ",(0,s.jsx)(t.em,{children:"right"})," image even if the words in the query aren\u2019t an exact match to the caption."]}),"\n",(0,s.jsx)(t.h2,{id:"the-tech-stack--pipeline",children:"The Tech Stack & Pipeline"}),"\n",(0,s.jsx)(t.p,{children:"Here\u2019s how it all fits together:"}),"\n",(0,s.jsx)(t.h3,{id:"step-1-captioning-with-florence-2",children:"Step 1: Captioning with Florence 2"}),"\n",(0,s.jsxs)(t.p,{children:["We start with raw images. ",(0,s.jsx)(t.a,{href:"https://huggingface.co/microsoft/Florence-2-base",children:"Florence 2"}),", a vision-language model by Microsoft, generates descriptive captions for each one. These captions help both with embedding generation and keyword search later on."]}),"\n",(0,s.jsx)(t.p,{children:"For example, an image of a storm in Chicago might be captioned as:"}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.em,{children:'"A wet intersection of cars with a sky filled with dark clouds."'})}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"step-2-embedding-with-imagebind",children:"Step 2: Embedding with ImageBind"}),"\n",(0,s.jsxs)(t.p,{children:["We then use ",(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.a,{href:"https://imagebind.metademolab.com",children:"ImageBind"})}),", a multi-modal embedding model by Meta, to generate vector representations \u2014 not just for the images, but for the generated captions as well. The key is that ImageBind places all these different modalities (images, text, audio, etc.) into the ",(0,s.jsx)(t.strong,{children:"same vector space"}),". This enables meaningful similarity comparisons."]}),"\n",(0,s.jsx)(t.h3,{id:"step-3-vector--keyword-search",children:"Step 3: Vector + Keyword Search"}),"\n",(0,s.jsx)(t.p,{children:"At query time, we do two types of search:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Vector Search:"})," The user\u2019s query is embedded into the same space, and we search for the most similar image-caption vectors."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Keyword Search:"})," The query is also run as a traditional text search against the metadata and captions using ",(0,s.jsx)(t.strong,{children:"BM25"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["Each method has its strengths. Vector search understands ",(0,s.jsx)(t.em,{children:"meaning"}),", while keyword search catches ",(0,s.jsx)(t.em,{children:"literal matches"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"step-4-hybrid-search-fusion",children:"Step 4: Hybrid Search Fusion"}),"\n",(0,s.jsxs)(t.p,{children:["We merge the results from both search types by calculating a weighted score based on a ",(0,s.jsx)(t.a,{href:"https://weaviate.io/blog/hybrid-search-fusion-algorithms",children:"fusion algorithm"}),". This hybrid strategy balances ",(0,s.jsx)(t.strong,{children:"semantic relevance"})," from vector search with the ",(0,s.jsx)(t.strong,{children:"precision"})," of keyword matching, ensuring more accurate and meaningful results."]}),"\n",(0,s.jsx)(t.h3,{id:"step-5-reranking-with-ms-marco-minilm-l6-v2",children:"Step 5: Reranking with ms-marco-MiniLM-L6-v2"}),"\n",(0,s.jsxs)(t.p,{children:["Now we pass all those results through a ",(0,s.jsx)(t.strong,{children:"reranker"})," \u2014 a small but powerful transformer model trained on the ",(0,s.jsx)(t.a,{href:"https://github.com/microsoft/MSMARCO-Passage-Ranking",children:"MS MARCO"})," dataset. It re-evaluates how well each result matches the original query, using a deeper level of semantic understanding, and reorders them accordingly."]}),"\n",(0,s.jsx)(t.p,{children:"This is the finishing touch that helps boost the most relevant images to the top."}),"\n",(0,s.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(t.p,{children:"There\u2019s a lot of exciting ground to cover next. Some upcoming improvements include:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Better Captions"}),": Experimenting with prompts or other models to produce more concise, relevant captions."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Tuning Hybrid Weighting"}),": Adjusting how much influence each search type has in the final result."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Fine-tuning the Reranker"}),": Custom training the reranker could further improve accuracy."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"User Interface"}),": Building a front-end interface to make this interactive and visually intuitive."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"why-this-matters",children:"Why This Matters"}),"\n",(0,s.jsxs)(t.p,{children:["As our object database continues to grow, the challenge of finding the right image at the right moment becomes increasingly difficult. With thousands of visuals available, building rich datasets with Sage is possible \u2014 but navigating that volume can be frustrating and time-consuming. Traditional search methods in our cyberinfrastructure are starting to show their limits. ",(0,s.jsx)(t.strong,{children:"Time-based filtering"})," assumes users already know when the relevant images were captured, while ",(0,s.jsx)(t.strong,{children:"metadata filtering"})," often forces them to sift through large volumes manually to find what they need. That\u2019s where this ",(0,s.jsx)(t.strong,{children:"hybrid approach"})," makes a real difference \u2014 by combining the ",(0,s.jsx)(t.strong,{children:"semantic power of vector search"})," with the ",(0,s.jsx)(t.strong,{children:"precision of keyword matching"}),", users can efficiently locate the exact image they\u2019re looking for \u2014 and pinpoint the precise moment it was captured."]}),"\n",(0,s.jsx)(t.p,{children:"If you\u2019re working on something similar or curious to dive deeper, feel free to reach out."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://github.com/waggle-sensor/sage-vectordb-example/tree/main/HybridSearch_example",children:"Browse the project code on GitHub"})})]})}function g(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);