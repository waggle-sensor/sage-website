"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8663],{27748:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/sound-separation-3-ed6cd7ce8d19c15a08c73d6b4ab1ea38.png"},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(96540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},60869:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"recent/sound-separation","title":"Sound Separation","description":"By processing data directly at the collection source, edge computing networks like Sage/Waggle offer unique advantages over using centralized servers for ecological monitoring tasks. Audio in particular is useful for tracking metrics like noise pollution from human development or population statistics for vocal wildlife in the area (birds, insects, frogs, etc.). Further analysis can provide deeper insights into the overall health of the ecosystems where nodes are deployed.","source":"@site/science/recent/sound-separation.md","sourceDirName":"recent","slug":"/recent/sound-separation","permalink":"/science/recent/sound-separation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Resource Management at the Edge","permalink":"/science/recent/resource-management"},"next":{"title":"Super Resolution Image Enhancement","permalink":"/science/recent/super-resolution"}}');var t=i(74848),s=i(28453);const o={sidebar_position:2},a="Sound Separation",d={},c=[{value:"Mixture Invariant Training (MixIT)",id:"mixture-invariant-training-mixit",level:2},{value:"Perch",id:"perch",level:3},{value:"Future Work",id:"future-work",level:2},{value:"Data Preprocessing",id:"data-preprocessing",level:3},{value:"Federated Learning",id:"federated-learning",level:3},{value:"References",id:"references",level:2}];function l(e){const n={em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"sound-separation",children:"Sound Separation"})}),"\n",(0,t.jsx)(n.p,{children:"By processing data directly at the collection source, edge computing networks like Sage/Waggle offer unique advantages over using centralized servers for ecological monitoring tasks. Audio in particular is useful for tracking metrics like noise pollution from human development or population statistics for vocal wildlife in the area (birds, insects, frogs, etc.). Further analysis can provide deeper insights into the overall health of the ecosystems where nodes are deployed."}),"\n",(0,t.jsx)(n.p,{children:"However, processing this data comes with its own challenges. Recordings made in the field consequently include environmental background noise and many overlapping sound sources present during the same time and within the same frequency range, making it difficult to parse for valuable information. This project explores the application of a sound separation model (MixIT [1]) to remedy these issues."}),"\n",(0,t.jsx)(n.h2,{id:"mixture-invariant-training-mixit",children:"Mixture Invariant Training (MixIT)"}),"\n",(0,t.jsx)(n.p,{children:"The motivation behind using MixIT over any other sound separation model is that fact that it is self-supervised. In contrast to traditional supervised methods, training for a MixIT model can be performed with entirely unlabeled data. This is appealing as it means we can train a model for focused use in a specific ecosystem without expending resources to manually create a labeled dataset beforehand."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MixIT model architecture",src:i(79574).A+"",width:"864",height:"266"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fig. 1."})," ",(0,t.jsx)(n.em,{children:"MixIT model architecture [1]"})]}),"\n",(0,t.jsx)(n.h3,{id:"perch",children:"Perch"}),"\n",(0,t.jsx)(n.p,{children:"One of the MixIT models we tested is part of another project called Perch [2]. It was pretrained on birdsong recordings. Given that the input data from Morton Arboretum consists mostly of the same, the model performed quite well."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"sample 1 spectrograms from Perch inference results",src:i(70253).A+"",width:"1636",height:"1189"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fig. 2."})," ",(0,t.jsx)(n.em,{children:"Spectrograms processed by the 8-output Perch model. From the top: original field recording, road noise, bird 1, bird 2, a second channel of road noise, animal 1, wind noise, bird 3."})]}),"\n",(0,t.jsx)(n.p,{children:"In the sample shown above the Perch model separated out not only birds, but other sounds that it was not deliberately trained for such as noise from the road, wind, and some small animal moving around in the grass nearby. This interesting result is likely due to the self-supervised learning process picking up on background noise patterns that were common across many recordings in the original training dataset."}),"\n",(0,t.jsx)(n.h2,{id:"future-work",children:"Future Work"}),"\n",(0,t.jsx)(n.h3,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,t.jsx)(n.p,{children:"Although MixIT does not strictly require it, separation performace is improved significantly by taking some time to preprocess training data. One approach could be to strategically slice recordings for relative peaks in volume and/or frequency, as shown below. This would counteract the disproportionate amount of background noise present in field recordings and minimize wasted data."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"preliminary results from peak-detection script",src:i(27748).A+"",width:"640",height:"480"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fig. 3"})," ",(0,t.jsx)(n.em,{children:"Preliminary results from peak-detection script"})]}),"\n",(0,t.jsx)(n.h3,{id:"federated-learning",children:"Federated Learning"}),"\n",(0,t.jsx)(n.p,{children:"The long-term goal of this project is to implement a MixIT separation model as part of a federated learning network at the edge. In such a system, nodes would use their onboard GPUs to collaboratively train a single model from the data their own sensors are collecting. Model updates are sent upstream to a central server, which then combines changes and sends the full model back to each node to continue the process. This provides numerous advantages, including reducing strain on each individual node while possibly improving inference performance of the resulting model."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:'Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron J. Weiss, Kevin Wilson, John R.\nHershey, "Unsupervised Sound Separation Using Mixture Invariant Training", Advances in\nNeural Information Processing Systems, 2020.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:'Tom Denton, Scott Wisdom, John R. Hershey, "Improving Bird Classification with\nUnsupervised Sound Separation", Proc. IEEE International Conference on Audio, Speech, and\nSignal Processing (ICASSP), 2022.'}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},70253:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/sound-separation-2-e4691ddd85dcf55844f54e26e228c161.png"},79574:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/sound-separation-1-b08749689d8744c579d02e109088f324.png"}}]);