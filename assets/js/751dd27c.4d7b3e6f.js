"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9552],{4498:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/DDCsim2results-879d4c789ae05d9aa000c46dc757e7ab.png"},22999:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/DDCdeer-2399835d706dc2d52ebe2fb420be4ab4.jpg"},27815:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/DDCsim1results-411a0cd95010652998847e7d97b837ba.png"},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>o});var i=n(96540);const s={},a=i.createContext(s);function r(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:t},e.children)}},37928:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/DDCsim2pic-e9dabf583bfbd50b1ab9be8abd95155d.png"},41226:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"recent/dynamic-data-collection","title":"Application-Agnostic Dynamic Data Collection for AI on the Edge","description":"Henry Abrahamson","source":"@site/science/recent/dynamic-data-collection.md","sourceDirName":"recent","slug":"/recent/dynamic-data-collection","permalink":"/science/recent/dynamic-data-collection","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"keywords":["Dynamic Sensing","Autonomous Sensor Networks","Modelling"],"sidebar_label":"Application-Agnostic Dynamic Data Collection"},"sidebar":"tutorialSidebar","previous":{"title":"Recent projects","permalink":"/science/category/recent-projects"},"next":{"title":"Autonomous Camera Control","permalink":"/science/recent/autonomous-camera-control"}}');var s=n(74848),a=n(28453);const r={sidebar_position:0,keywords:["Dynamic Sensing","Autonomous Sensor Networks","Modelling"],sidebar_label:"Application-Agnostic Dynamic Data Collection"},o="Application-Agnostic Dynamic Data Collection for AI on the Edge",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Motivation",id:"motivation",level:3},{value:"Our Work",id:"our-work",level:3},{value:"Modelling",id:"modelling",level:2},{value:"Environment Model",id:"environment-model",level:3},{value:"Strategy",id:"strategy",level:3},{value:"Simulation Results",id:"simulation-results",level:2},{value:"Responsive Strategy + Markovian Environment",id:"responsive-strategy--markovian-environment",level:3},{value:"Waiting Time Strategy + Preset Environment with More Realistic Numbers",id:"waiting-time-strategy--preset-environment-with-more-realistic-numbers",level:3},{value:"Future Directions",id:"future-directions",level:2}];function h(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"application-agnostic-dynamic-data-collection-for-ai-on-the-edge",children:"Application-Agnostic Dynamic Data Collection for AI on the Edge"})}),"\n",(0,s.jsx)(t.p,{children:"Henry Abrahamson"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.em,{children:"Northwestern Unversity, Department of Electrical and Computer Engineering"})}),"\n",(0,s.jsxs)(t.p,{children:["email: ",(0,s.jsx)(t.a,{href:"mailto:henryabrahamson2022@u.northwestern.edu",children:"henryabrahamson2022@u.northwestern.edu"})]}),"\n",(0,s.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(t.h3,{id:"motivation",children:"Motivation"}),"\n",(0,s.jsxs)("div",{className:"md:float-right md:ml-6 md:max-w-xs",children:[(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"deer picture",src:n(22999).A+"",width:"1280",height:"854"})}),(0,s.jsx)("small",{className:"flex align-center justify-end mt-[-25px] text-slate-500",children:(0,s.jsxs)(t.p,{children:["(",(0,s.jsx)(t.a,{href:"https://heritageconservancy.org/wp-content/uploads/2023/11/deer-8175966_1280.jpg",children:"Image Source"}),")"]})})]}),"\n",(0,s.jsxs)(t.p,{children:["Consider a scientific application that uses a SAGE node to capture video in the wild in the hopes of using an AI model to identify the deer and describe what it is doing. In this application, there are large periods of inactivity when the camera captures no deer or other animals, punctuated by short periods of interest when a deer is on camera. In this way, the environment is ",(0,s.jsx)(t.em,{children:"dynamic"}),". However, if SAGE naively runs this job ",(0,s.jsx)(t.em,{children:"statically"}),", without any changes in response to the environment, multiple potential problems arise:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Because the job relies on video or other high-memory data, the data collected at the edge could exceed the SAGE node's ability to transmit it back."}),"\n",(0,s.jsx)(t.li,{children:"Despite generating so much data, most of it is irrelevant or has few features of interest."}),"\n",(0,s.jsx)(t.li,{children:"If this job is being run sparsely in time, it may miss a deer sighting entirely, or to only run the job once during a sighting when it may be possible to run it multiple times."}),"\n",(0,s.jsx)(t.li,{children:"On the other hand, the job cannot be run at all times, either because other scientific applications have reserved time on the SAGE node, or because resources like power are scarce and have to be appropriately rationed."}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"our-work",children:"Our Work"}),"\n",(0,s.jsxs)(t.p,{children:["All of these problems are not unique to our hypothetical deer-related application, but apply to a wide variety of tasks submitted to SAGE. Hence, we devise a ",(0,s.jsx)(t.strong,{children:"framework for dynamic data-collection strategies for AI applications on the edge"}),". Our framework must have the following two properties:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Generality:"})," There should be no application-specific dependencies in our framework, so that it has broad applicability for future jobs."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Edge-Implementability:"})," The strategies that our framework covers should only have to rely on local decision-making, to ensure prompt reactions to the dynamic environment."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["In order to explore our framework, we also create a ",(0,s.jsx)(t.strong,{children:"simulation"}),". This simulation is written so that SAGE users interested in a dynamic data collection strategy for their job can freely explore how different strategies might affect their output data, using parameters and features motivated by their specific application. You can find the simulator here:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://github.com/waggle-sensor/summer2025/blob/main/henry/CollectionStrategySimulation.ipynb",children:"Dynamic Data Collection Simulator"})," ",(0,s.jsx)(t.a,{href:"https://colab.research.google.com/github/waggle-sensor/summer2025/blob/main/henry/CollectionStrategySimulation.ipynb",children:(0,s.jsx)(t.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"open in google colab"})})]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"modelling",children:"Modelling"}),"\n",(0,s.jsxs)(t.p,{children:["Our framework consists of two components: the ",(0,s.jsx)(t.strong,{children:"environment"}),", and the ",(0,s.jsx)(t.strong,{children:"strategy"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"environment-model",children:"Environment Model"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"environmental state diagram",src:n(75750).A+"",width:"1176",height:"610"})}),"\n",(0,s.jsxs)(t.p,{children:["Image Source ",(0,s.jsx)(t.a,{href:"https://www.needpix.com/photo/download/1741227/forest-fire-wildfire-blaze-firefighters-smoke-trees-heat-burning-hot",children:"1"}),", ",(0,s.jsx)(t.a,{href:"https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/7/78/Chacachacare_dry_forest_3.JPG",children:"2"})]}),"\n",(0,s.jsxs)(t.p,{children:["The environment controls what type of data the strategy can get when it tries to collect a sample. It is modelled using ",(0,s.jsx)(t.strong,{children:"two states"}),": an ",(0,s.jsx)(t.em,{children:"active"})," state in which is it tends to produce more ",(0,s.jsx)(t.em,{children:"relevant"})," than ",(0,s.jsx)(t.em,{children:"irrelevant"})," data (in orange above), and a ",(0,s.jsx)(t.em,{children:"passive"})," state, which produces the opposite (in blue). Essentially, the ",(0,s.jsx)(t.em,{children:"active"})," state corresponds to an environment in which an event of interest is occurring, while the ",(0,s.jsx)(t.em,{children:"passive"})," state corresponds to an environment where nothing of note is occurring."]}),"\n",(0,s.jsxs)(t.p,{children:["Both the proportions of ",(0,s.jsx)(t.em,{children:"relevant"})," vs. ",(0,s.jsx)(t.em,{children:"irrelevant"})," data generated by an ",(0,s.jsx)(t.em,{children:"active"})," vs. ",(0,s.jsx)(t.em,{children:"passive"})," environment, as well as the transitions between the ",(0,s.jsx)(t.em,{children:"active"})," and ",(0,s.jsx)(t.em,{children:"passive"})," states are changeable based on the specific application we are interested in investigating. We implemented the following environmental models:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"One-State:"})," A static, one-state environment, used mainly for comparison."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Markovian:"})," A simple 2-state Markov chain. Transition probabilities are tunable parameters."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Time Correlations:"})," A preset version of the Markovian environment. It still has the same two states and generates data stochastically, but instead of having transition probabilities, it instead is given a set of time-ranges during which the environment is ",(0,s.jsx)(t.em,{children:"active"}),", and outside of which it is ",(0,s.jsx)(t.em,{children:"passive"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Preset:"})," A fully deterministic environment that will always produce ",(0,s.jsx)(t.em,{children:"relevant"})," data in the ",(0,s.jsx)(t.em,{children:"active"})," state, and ",(0,s.jsx)(t.em,{children:"irrelevant"})," data in the ",(0,s.jsx)(t.em,{children:"passive"})," state. The environment's states are determined by a user inputted array of labels. Particularly useful if the user already has access to a sample run of their job or a similar job run on SAGE."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"strategy",children:"Strategy"}),"\n",(0,s.jsxs)(t.p,{children:["The strategy controls how samples are generated. As with the environment, the strategy also has two states, ",(0,s.jsx)(t.em,{children:"active"})," and ",(0,s.jsx)(t.em,{children:"passive"}),". In each state, the strategy can control two parameters:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Sample Rate"}),": How frequently samples are generated, or how often a job is run."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Quality Level"}),": Whether a sample is considered to be high quality or not. This attribute is a little more abstract, but it could represent a variety of changes that make the resulting data more desirable, such as using a higher-end, more expensive AI model instead of a cheaper one, or using a camera's zoom so the video frame has less dead space and is focused on the object of interest."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Hence, a common strategy would be to take low-quality but frequent samples in the passive state, and then take high-quality samples while in the active state."}),"\n",(0,s.jsx)(t.p,{children:"Also as before, the transitions between states are mutable. We explored two strategies:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Responsive:"})," This strategy transitions to ",(0,s.jsx)(t.em,{children:"active"})," once it senses a piece of ",(0,s.jsx)(t.em,{children:"relevant"})," data. It then transitions down to ",(0,s.jsx)(t.em,{children:"passive"})," after it detects a number of consecutive ",(0,s.jsx)(t.em,{children:"irrelevant"})," data points. The number needed is tunable. Applies neatly to a variety of applications."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Waiting Time:"})," This strategy transitions to ",(0,s.jsx)(t.em,{children:"active"})," once it senses a piece of ",(0,s.jsx)(t.em,{children:"relevant"})," data. It then remains in the ",(0,s.jsx)(t.em,{children:"active"})," state for a number of samples (tunable), at which point it transitions back to the ",(0,s.jsx)(t.em,{children:"passive"})," state if its most recent data point was ",(0,s.jsx)(t.em,{children:"irrelevant"}),", otherwise, it stays ",(0,s.jsx)(t.em,{children:"active"})," for that number of samples again. Useful if the application intends to run multiple AI models at different time scales at the same time."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"simulation-results",children:"Simulation Results"}),"\n",(0,s.jsx)(t.p,{children:"As a proof of concept, we ran the simulation under two conditions."}),"\n",(0,s.jsx)(t.h3,{id:"responsive-strategy--markovian-environment",children:"Responsive Strategy + Markovian Environment"}),"\n",(0,s.jsxs)(t.p,{children:["Firstly, without any application in mind for motivation, we used the Responsive strategy that can switch between high and low quality measurements, with a static sample rate of 1 sample per second and a memory cap (",(0,s.jsx)(t.em,{children:"m"}),") of 1 vs. 4 for comparison, alongside a Markovian environment with parameters as in the figure below. We ran the simulation for 500  seconds in simulation time."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"simenvironment1",src:n(70090).A+"",width:"1060",height:"526"})}),"\n",(0,s.jsxs)(t.p,{children:["We see how our strategy responds to the environment over time in the figure below. The gray line indicates the measurement quality produced by the strategy, while the black line indicates the relevancy of the sample produced by the environment. Ideally, the gray line should follow the black line closely. We see that both the ",(0,s.jsx)(t.em,{children:"m"})," = 1 and 4 strategies track the environment reasonably well, with the ",(0,s.jsx)(t.em,{children:"m"})," = 1 measurement quality being generally more volatile since it tries to track the sample relevancy exactly. On the other hand, the ",(0,s.jsx)(t.em,{children:"m"})," = 4 strategy is smoother, since it waits for 4 time steps before switching from active to passive sensing. As such, the ",(0,s.jsx)(t.em,{children:"m"})," = 1 strategy has fewer high quality measurements wasted on irrelevant data points, but the ",(0,s.jsx)(t.em,{children:"m"})," = 4 strategy has more high quality measurements of relevant data, since it has fewer misses from switching to passive sensing too early."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"sim1results",src:n(27815).A+"",width:"1246",height:"538"})}),"\n",(0,s.jsxs)(t.p,{children:["Assigning a cost of 1 Joule to passive sensing and 3 Joules to active sensing, we see that the ",(0,s.jsx)(t.em,{children:"m"})," = 1 strategy costs 620J and the ",(0,s.jsx)(t.em,{children:"m"})," = 4 strategy costs 678J, compared to a baseline of 500J for an all-low-quality strategy and 1500J for an all-high-quality strategy. Thus for ",(0,s.jsx)(t.em,{children:"m"})," = 4, we incur a penalty of 35% extra energy to capture most of the relevant data with high quality measurements, compared to the 200% penalty from the all-high-quality strategy."]}),"\n",(0,s.jsx)(t.h3,{id:"waiting-time-strategy--preset-environment-with-more-realistic-numbers",children:"Waiting Time Strategy + Preset Environment with More Realistic Numbers"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"sim2pic",src:n(37928).A+"",width:"674",height:"505"})}),"\n",(0,s.jsxs)(t.p,{children:["Next, we devise a simple but real application to have the simulation generate realistic numbers, and to see if we still have tangible gains. In this case, we assume that a SAGE node is posted by a road, and is taking a picture once per minute and analyzing is using ",(0,s.jsx)(t.a,{href:"https://docs.ultralytics.com/tasks/detect/",children:"YOLO"})," to detect cars as they drive by. Once YOLO detects a car, the next picture taken is analyzed using the more computationally expensive ",(0,s.jsx)(t.a,{href:"https://moondream.ai/",children:"Moondream"})," for a more detailed description of the scene. Based on previous SAGE usage of Moondream, we assume that it takes 10 minutes for it to run to completion (counting initialization time). While Moondream is running, YOLO still runs once per minute in the background. We also assume based on past runs of these jobs that both jobs take up 1 CPU and GPU per minute."]}),"\n",(0,s.jsx)(t.p,{children:"Using our framework, we use the Waiting Time strategy, assuming that the strategy can only transition once per minute. The passive state is the state in which only YOLO is running, with an associated cost of 1 CPU minute per minute, while the active state has both YOLO and Moondream running with an associated cost off 2 CPU minutes per minute. The Waiting Time strategy's waiting parameter is 10 minutes, since that's how long it takes for Moondream, the longer of the two jobs, to run."}),"\n",(0,s.jsx)(t.p,{children:"In this scenario, YOLO can be thought of as a tripwire: most of the time, the SAGE node runs the relatively quick YOLO job. Once YOLO detects something of interest though, the node then invokes the more expensive Moondream job. By using this dynamic strategy the node saves power in comparison to running YOLO and Moondream all the time."}),"\n",(0,s.jsx)(t.p,{children:"For the environment, we collected video from one of the SAGE nodes near argonne campus, manually labelled the frames as relevant / irrelevant if there was a car in focus on the road, and loaded that data in as a Preset environment."}),"\n",(0,s.jsx)(t.p,{children:"The results are shown in the table below. Using our dynamic strategy, we were able to get 27 runs of Moondream on relevant data, compared to 15 runs by naively running Moondream once every 10 minutes. This jump occurs because many of the events of interest lasted fewer than 10 minutes -- as such, using YOLO as a trigger allows the SAGE node to notice these short events and respond appropriately."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"sim2table",src:n(4498).A+"",width:"1782",height:"448"})}),"\n",(0,s.jsx)(t.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsxs)(t.p,{children:["Potential directions for future work include the implementation of these strategies on a SAGE node directly, developing new environment and strategy models (of particular interest would be a trinary state strategy with states representing ",(0,s.jsx)(t.em,{children:"passive"}),", ",(0,s.jsx)(t.em,{children:"ready"}),", and ",(0,s.jsx)(t.em,{children:"active"}),"). These two thrusts would both move towards making these dynamic data collection closer to being applied in reality and hopefully more easily accessible to SAGE users. Also of interest would be obtaining theoretical guarantees of performance given a Markovian environment with bounded parameters, providing a mathematical justification for these strategies, and allowing for the development of a tool that might be able to calculate worst-case performances."]})]})}function d(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},70090:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/DDCsim1state-cb5f89fd7fbc73e5796c5e23624b4f48.png"},75750:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/DDCstatepic-a537b9394cffb92fe56e86ccae005168.png"}}]);