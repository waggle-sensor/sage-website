"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3611],{14960:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step1-logs-2f69337a6bf6f2a3e1371d045491a916.png"},16972:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step2-plant_detected-4b33f2a9938a80292a1055b0982c15fd.png"},23770:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step2-image_capture-09836af5f4da391f9aab3aac197eb601.png"},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var s=t(96540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}},32124:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/PTZ-pipeline-539ca2a53fdcbc2fb4d130786f74aec5.png"},39824:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step3-final_shot-a1a87a66ca158e5f841785342b9d2ea3.png"},41513:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"type":"mdx","permalink":"/labs/ptz-yolo","source":"@site/src/pages/labs/ptz-yolo.md","title":"PTZApp: Smarter Cameras for Wildlife and Scene Monitoring","description":"Whether you\'re monitoring wildlife, securing an area, or collecting snapshots from dynamic outdoor scenes, this application helps you do it automatically, intelligently, and efficiently.","frontMatter":{},"unlisted":false}');var i=t(74848),o=t(28453);const a={},l="PTZApp: Smarter Cameras for Wildlife and Scene Monitoring",r={},c=[{value:"What We&#39;re Building",id:"what-were-building",level:2},{value:"Supported Models",id:"supported-models",level:2},{value:"YOLO (Default)",id:"yolo-default",level:3},{value:"Florence-2",id:"florence-2",level:3},{value:"Workflow: The Intelligent Cascade",id:"workflow-the-intelligent-cascade",level:2},{value:"Where It Runs",id:"where-it-runs",level:2},{value:"Key Features &amp; Data Publishing",id:"key-features--data-publishing",level:2},{value:"Best-of-N Capture with a Blur Gate",id:"best-of-n-capture-with-a-blur-gate",level:3},{value:"Metadata Logging &amp; Telemetry",id:"metadata-logging--telemetry",level:3},{value:"The Alert System",id:"the-alert-system",level:3},{value:"How to Run",id:"how-to-run",level:2},{value:"Docker (Recommended for GPU servers)",id:"docker-recommended-for-gpu-servers",level:3},{value:"Python directly (For development)",id:"python-directly-for-development",level:3},{value:"Tunables &amp; Flags",id:"tunables--flags",level:2},{value:"What You\u2019ll See (Outputs)",id:"what-youll-see-outputs",level:2},{value:"Step 1: Scene Captioning",id:"step-1-scene-captioning",level:3},{value:"Step 2: Plant Detection and Workflow Trigger",id:"step-2-plant-detection-and-workflow-trigger",level:3},{value:"Step 3: Auto-Zoom and Best-of-N Capture",id:"step-3-auto-zoom-and-best-of-n-capture",level:3},{value:"Step 4: Final Species Identification",id:"step-4-final-species-identification",level:3},{value:"Why It Matters: Scientific &amp; Environmental Use",id:"why-it-matters-scientific--environmental-use",level:2},{value:"Limitations &amp; Notes",id:"limitations--notes",level:2},{value:"What\u2019s Coming Next",id:"whats-coming-next",level:2},{value:"Get Involved",id:"get-involved",level:2}];function d(e){const n={blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ptzapp-smarter-cameras-for-wildlife-and-scene-monitoring",children:"PTZApp: Smarter Cameras for Wildlife and Scene Monitoring"})}),"\n",(0,i.jsx)(n.p,{children:"Whether you're monitoring wildlife, securing an area, or collecting snapshots from dynamic outdoor scenes, this application helps you do it automatically, intelligently, and efficiently."}),"\n",(0,i.jsx)(n.p,{children:"This article walks through this new architecture, its advanced capabilities, and how it enables more targeted and meaningful environmental monitoring."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"what-were-building",children:"What We're Building"}),"\n",(0,i.jsx)(n.p,{children:"This application transforms a standard PTZ camera into a smart, self-directing observation system. It has evolved beyond a simple scanner to a context-aware agent."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," The PlantNet integration below is an ",(0,i.jsx)(n.em,{children:"example use case"})," if you are exploring the plant detection path. It demonstrates how PTZ-YOLO\u2019s detection and zooming capabilities can be extended for species identification and environmental analysis. It is not a required component of the PTZ-YOLO core functionality."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The application:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understands the scene"})," using a high-level captioning model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performs a contextual scan"})," to find objects relevant to the scene's description."]}),"\n",(0,i.jsxs)(n.li,{children:["Detects objects using models like ",(0,i.jsx)(n.strong,{children:"YOLO"})," or the more advanced ",(0,i.jsx)(n.strong,{children:"Florence-2"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Automatically ",(0,i.jsx)(n.strong,{children:"centers and zooms"})," to get the best shot of a target."]}),"\n",(0,i.jsxs)(n.li,{children:["Optionally performs ",(0,i.jsx)(n.strong,{children:"plant species identification"}),' via PlantNet, using the sharpest image from a "best-of-N" capture sequence.']}),"\n",(0,i.jsxs)(n.li,{children:["Publishes rich ",(0,i.jsx)(n.strong,{children:"metadata, logs, and high-priority alerts"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Runs autonomously on the edge (no cloud inference required)."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,i.jsx)(n.p,{children:"The application supports two model types depending on the use case:"}),"\n",(0,i.jsx)(n.h3,{id:"yolo-default",children:"YOLO (Default)"}),"\n",(0,i.jsx)(n.p,{children:"A lightweight and fast model for known, predefined categories. Ideal for speed and efficiency when you know what you're looking for."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'--model yolo11n --objects "person,car,dog"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"florence-2",children:"Florence-2"}),"\n",(0,i.jsx)(n.p,{children:'A powerful vision-language model that enables the new "Intelligent Cascade" workflow. It\'s used for a sophisticated two-stage process that goes far beyond simple object detection:'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Scene Captioning (",(0,i.jsx)(n.code,{children:"<MORE_DETAILED_CAPTION>"}),"):"]})," The model first generates a high-level, descriptive caption to understand the entire scene."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Contextual Grounding (",(0,i.jsx)(n.code,{children:"<CAPTION_TO_PHRASE_GROUNDING>"}),"):"]}),' This caption is then used as context for the object detection prompt. The model is no longer just asked to find a "plant," but to find a "plant" ',(0,i.jsx)(n.em,{children:"in the context of the scene it just described"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example (Detect specific plants):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'--model Florence-base --objects "plant,tree,bush,wildflower"\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example (Detect everything visible):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'--model Florence-base --objects "*"\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"workflow-the-intelligent-cascade",children:"Workflow: The Intelligent Cascade"}),"\n",(0,i.jsx)(n.p,{children:"The application's architecture has been redesigned from a rigid, hardcoded process to a flexible, context-aware workflow. This shift allows the camera to make smarter decisions about what is important in a scene."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Architecture diagram of the Intelligent Cascade workflow",src:t(32124).A+"",width:"794",height:"1582"})}),"\n",(0,i.jsx)(n.p,{children:'Here is how the new "Intelligent Cascade" operates:'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scene Captioning"}),": The process begins with Florence-2 generating a detailed caption of the entire scene to create a foundational understanding."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contextual Scan & Phrase Grounding"}),": Instead of a blind 360\xb0 scan, the system uses the scene caption to inform its search for objects within that specific context."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decision Loop"}),": Based on the objects found, the system enters a decision loop to trigger different actions based on node-specific logic or object type."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Publish Based on Decision"}),": Only after a decision is made does the system act. Actions can include zooming in, passing the image to PlantNet, or publishing the image and all associated metadata."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"where-it-runs",children:"Where It Runs"}),"\n",(0,i.jsx)(n.p,{children:"PTZApp is containerized and ready for deployment across different platforms:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU Servers"})," (e.g. Dell blades) for high-throughput inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Waggle Nodes"})," (Jetson Xavier or AGX Orin) for sensor-rich environments in remote areas"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-features--data-publishing",children:"Key Features & Data Publishing"}),"\n",(0,i.jsx)(n.h3,{id:"best-of-n-capture-with-a-blur-gate",children:"Best-of-N Capture with a Blur Gate"}),"\n",(0,i.jsx)(n.p,{children:"To ensure high-quality images for species identification, the app captures 1\u20133 quick shots and scores each for sharpness. If the best image is still too soft (blurry), it performs a quick focus adjustment and retries once, ensuring PlantNet receives a clear image."}),"\n",(0,i.jsx)(n.h3,{id:"metadata-logging--telemetry",children:"Metadata Logging & Telemetry"}),"\n",(0,i.jsx)(n.p,{children:"The original pipeline only published an image, losing valuable intermediate data. The new system logs all reasoning and publishes multiple distinct data streams:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ptz.scene.caption"})}),": The overall scene context generated by Florence-2."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ptz.detection.*"})}),": A JSON payload with the label, confidence, bounding box, and camera PTZ coordinates for every detection."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ptz.plantnet.species"})}),": The full JSON result from the PlantNet API."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"the-alert-system",children:"The Alert System"}),"\n",(0,i.jsxs)(n.p,{children:["The system can now actively monitor for important events. Using configurable lists of species (e.g., ",(0,i.jsx)(n.code,{children:"invasive_species.json"}),"), it checks the PlantNet results. If a species of interest is detected, it publishes a ",(0,i.jsx)(n.strong,{children:"high-priority alert"})," (e.g., ",(0,i.jsx)(n.code,{children:"ptz.alert.invasive_species"}),")."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"how-to-run",children:"How to Run"}),"\n",(0,i.jsx)(n.h3,{id:"docker-recommended-for-gpu-servers",children:"Docker (Recommended for GPU servers)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"docker run --rm -it --network host \\\n  -v \"$(pwd)/imgs\":/imgs \\\n  -v hf_cache:/hf_cache \\\n  -e PLANTNET_API_KEY='<your_plantnet_api_key>' \\\n  -e BLUR_MIN=120 \\\n  -e SPECIES_MIN_SCORE=0.25 \\\n  saumyap29/ptzapp:latest \\\n    -m Florence-base \\\n    -obj \"plant,tree,bush,wildflower\" \\\n    -un <camera_user> -pw '<camera_pass>' -ip <camera_ip> \\\n    --species_zoom 10 --iterdelay 0 -it 1 --keepimages --debug\n"})}),"\n",(0,i.jsx)(n.h3,{id:"python-directly-for-development",children:"Python directly (For development)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export PLANTNET_API_KEY="<your_plantnet_api_key>"\npython main.py \\\n  -m Florence-base \\\n  -obj "plant,tree,bush,wildflower" \\\n  -un <camera_user> -pw \'<camera_pass>\' -ip <camera_ip> \\\n  --species_zoom 10 --iterdelay 0 -it 1 --keepimages --debug\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"tunables--flags",children:"Tunables & Flags"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--objects"}),": Comma-separated list of items to detect (e.g., ",(0,i.jsx)(n.code,{children:'"person,car"'}),"). Use ",(0,i.jsx)(n.code,{children:'"*"'})," with Florence to find all objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--confidence"}),": Detection confidence gate (default ",(0,i.jsx)(n.code,{children:"0.1"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--species_zoom"}),": Extra relative zoom applied to large plants before PlantNet capture (e.g., ",(0,i.jsx)(n.code,{children:"10"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"BLUR_MIN"})," (env): Sharpness threshold (Laplacian variance) for an image to be considered clear."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"SPECIES_MIN_SCORE"})," (env): Minimum PlantNet score required to publish a species result."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--keepimages"}),": Prevents deletion of saved images in the ",(0,i.jsx)(n.code,{children:"/imgs"})," directory."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--debug"}),": Enables verbose console logging for troubleshooting."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-see-outputs",children:"What You\u2019ll See (Outputs)"}),"\n",(0,i.jsx)(n.p,{children:"This section shows the step-by-step output flow from the application, from initial scene analysis to final species identification."}),"\n",(0,i.jsx)(n.h3,{id:"step-1-scene-captioning",children:"Step 1: Scene Captioning"}),"\n",(0,i.jsx)(n.p,{children:"The process begins with the camera taking a wide shot of the area. The Florence-2 model then generates a detailed text description of this image to establish context."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Initial Image:"}),"\n",(0,i.jsx)(n.img,{alt:"Initial wide shot of the scene",src:t(66399).A+"",width:"2018",height:"1204"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Console Log:"}),"\n",(0,i.jsx)(n.img,{alt:"Console log showing the generated Scene Context",src:t(14960).A+"",width:"2484",height:"922"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"step-2-plant-detection-and-workflow-trigger",children:"Step 2: Plant Detection and Workflow Trigger"}),"\n",(0,i.jsx)(n.p,{children:'Using the scene context, the model performs phrase grounding to locate objects. When it identifies an object with a plant-related label (like "trees"), it automatically triggers the species identification workflow.'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Image:"}),"\n",(0,i.jsx)(n.img,{alt:"shot of the particular detection",src:t(23770).A+"",width:"1900",height:"1108"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Console Log:"}),"\n",(0,i.jsx)(n.img,{alt:"Console log showing a plant has been detected",src:t(16972).A+"",width:"2090",height:"526"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"step-3-auto-zoom-and-best-of-n-capture",children:"Step 3: Auto-Zoom and Best-of-N Capture"}),"\n",(0,i.jsx)(n.p,{children:"The system automatically centers the camera on the detected plant and zooms in to get a better view. It takes multiple candidate shots (typically 3 tries) to ensure it can send the sharpest possible image to PlantNet."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Saved Candidate Images:"}),"\nA list of the three candidate images saved by the system with additional zoom in each"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example of a Zoomed-in Shot:"}),"\n",(0,i.jsx)(n.img,{alt:"A zoomed-in shot of the tree leaves",src:t(55123).A+"",width:"1980",height:"1116"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"step-4-final-species-identification",children:"Step 4: Final Species Identification"}),"\n",(0,i.jsx)(n.p,{children:"The sharpest image is sent to the PlantNet API. The final log shows the successful identification, including the scientific name, common names, and the confidence score."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Final Shot Sent:"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"The image sent to PlantNet for analysis",src:t(39824).A+"",width:"1834",height:"1040"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Console Log:"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Console log showing the final species identification results from PlantNet",src:t(53082).A+"",width:"2414",height:"518"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"why-it-matters-scientific--environmental-use",children:"Why It Matters: Scientific & Environmental Use"}),"\n",(0,i.jsx)(n.p,{children:"This intelligent approach to observation enables long-term, low-impact monitoring of ecological systems. This is ideal for:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Wildlife Monitoring"}),": Track animal and plant presence with targeted, high-quality snapshots without human disturbance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Biodiversity Studies"}),": Build rich, labeled datasets for conservation research and AI training."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Low-Bandwidth Operation"}),": Operate effectively in remote field locations by only uploading the most important moments and data."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Captured images are enriched with metadata like timestamps, PTZ coordinates, object labels, and\u2014when applicable\u2014plant species information."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"limitations--notes",children:"Limitations & Notes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["PlantNet accuracy depends heavily on image quality and the visibility of key organs (flowers, leaves). Adjust ",(0,i.jsx)(n.code,{children:"--species_zoom"})," and ",(0,i.jsx)(n.code,{children:"BLUR_MIN"})," for your specific camera and environment."]}),"\n",(0,i.jsx)(n.li,{children:"Night scenes or scenes with heavy motion may require higher camera settle delays or confidence thresholds."}),"\n",(0,i.jsx)(n.li,{children:"Florence-2 is more compute-intensive than YOLO; choose your model based on the device's capabilities and your detection needs."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"whats-coming-next",children:"What\u2019s Coming Next"}),"\n",(0,i.jsx)(n.p,{children:"The project has several exciting directions ahead:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Better Organ-Aware Framing"}),": Intelligently frame shots to focus on leaves, flowers, or fruit for more accurate species ID."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event-Based Triggers"}),": Use audio or motion sensors to trigger the camera, reducing power consumption and latency."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Camera Coordination"}),": Track a single object as it moves across the views of several cameras."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Active Learning"}),": Build systems to refine the detection models based on feedback from field data."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"get-involved",children:"Get Involved"}),"\n",(0,i.jsx)(n.p,{children:"If you are working on smart sensing, wildlife AI, or remote image collection, we invite you to explore this project. It is open, modular, and ready for collaboration. Contributions and issues are welcome."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},53082:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step3-species_detetcted_logs-ef75cd1903d92d0171411fd012edb573.png"},55123:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step2-zoomed_shot-1fa0a63fdd97323ef1fed4df15d4edc2.png"},66399:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/step1-initial_wideshot-1b6659e27f56a433140611f0e57c6736.png"}}]);