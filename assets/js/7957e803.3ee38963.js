"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3611],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var t=i(96540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},41513:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"type":"mdx","permalink":"/labs/ptz-yolo","source":"@site/src/pages/labs/ptz-yolo.md","title":"PTZ-YOLO: Smarter Cameras for Wildlife and Scene Monitoring","description":"In remote environments, every captured frame matters. What if your camera could not only look around but also decide where to look, what to zoom in on, and when to capture? The PTZ-YOLO application makes this possible by bringing Object Detection to PTZ (Pan-Tilt-Zoom) cameras deployed in the field.","frontMatter":{},"unlisted":false}');var r=i(74848),s=i(28453);const o={},l="PTZ-YOLO: Smarter Cameras for Wildlife and Scene Monitoring",a={},d=[{value:"What We&#39;re Building",id:"what-were-building",level:2},{value:"The Workflow",id:"the-workflow",level:2},{value:"Step 1: Initialization",id:"step-1-initialization",level:3},{value:"Step 2: Scanning",id:"step-2-scanning",level:3},{value:"Step 3: Detection",id:"step-3-detection",level:3},{value:"Step 4: Centering and Zooming",id:"step-4-centering-and-zooming",level:3},{value:"Step 5: Publishing",id:"step-5-publishing",level:3},{value:"Step 6: Iteration",id:"step-6-iteration",level:3},{value:"Why It Matters",id:"why-it-matters",level:2},{value:"Supported Models",id:"supported-models",level:2},{value:"YOLO (Default)",id:"yolo-default",level:3},{value:"Florence",id:"florence",level:3},{value:"Where It Runs",id:"where-it-runs",level:2},{value:"Scientific and Environmental Use",id:"scientific-and-environmental-use",level:2},{value:"What\u2019s Coming Next",id:"whats-coming-next",level:2},{value:"Get Involved",id:"get-involved",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ptz-yolo-smarter-cameras-for-wildlife-and-scene-monitoring",children:"PTZ-YOLO: Smarter Cameras for Wildlife and Scene Monitoring"})}),"\n",(0,r.jsx)(n.p,{children:"In remote environments, every captured frame matters. What if your camera could not only look around but also decide where to look, what to zoom in on, and when to capture? The PTZ-YOLO application makes this possible by bringing Object Detection to PTZ (Pan-Tilt-Zoom) cameras deployed in the field."}),"\n",(0,r.jsx)(n.p,{children:"Whether you're monitoring wildlife, securing an area, or collecting snapshots from dynamic outdoor scenes, PTZ-YOLO helps you do it automatically, intelligently, and efficiently."}),"\n",(0,r.jsx)(n.p,{children:"This article walks through how it works, what makes it effective, and where it is headed next."}),"\n",(0,r.jsx)(n.h2,{id:"what-were-building",children:"What We're Building"}),"\n",(0,r.jsx)(n.p,{children:"PTZ-YOLO transforms a standard PTZ camera into a smart, self-directing observation system. The application:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Scans its surroundings using pan, tilt, and zoom"}),"\n",(0,r.jsx)(n.li,{children:"Detects specific objects like people, cars, or deer using edge-deployed models"}),"\n",(0,r.jsx)(n.li,{children:"Centers and zooms in on detected subjects to optimize framing"}),"\n",(0,r.jsx)(n.li,{children:"Captures and publishes relevant images"}),"\n",(0,r.jsx)(n.li,{children:"Runs autonomously and continuously without needing a cloud to run inference"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This setup produces rich, targeted imagery while keeping bandwidth usage minimal."}),"\n",(0,r.jsx)(n.h2,{id:"the-workflow",children:"The Workflow"}),"\n",(0,r.jsx)(n.p,{children:"Here is how the PTZ-YOLO app operates:"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-initialization",children:"Step 1: Initialization"}),"\n",(0,r.jsx)(n.p,{children:"The app initializes the detection model (YOLO or Florence) and sets camera parameters such as pan step, tilt, and zoom."}),"\n",(0,r.jsx)(n.h3,{id:"step-2-scanning",children:"Step 2: Scanning"}),"\n",(0,r.jsx)(n.p,{children:"The camera performs a 360-degree scan by rotating in predefined pan steps (default is 15 degrees) at a specified tilt and zoom level."}),"\n",(0,r.jsx)(n.h3,{id:"step-3-detection",children:"Step 3: Detection"}),"\n",(0,r.jsx)(n.p,{children:"At each position, the system captures a frame and runs object detection. Detections below the confidence threshold are discarded."}),"\n",(0,r.jsx)(n.h3,{id:"step-4-centering-and-zooming",children:"Step 4: Centering and Zooming"}),"\n",(0,r.jsx)(n.p,{children:"If a target object is detected, the system:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Adjusts the pan and tilt to center the object"}),"\n",(0,r.jsx)(n.li,{children:"Changes the zoom to maximize the object's size in the frame"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-5-publishing",children:"Step 5: Publishing"}),"\n",(0,r.jsx)(n.p,{children:"Once optimized, the image is saved and optionally uploaded to cloud infrastructure such as Sage for archiving or real-time alerts."}),"\n",(0,r.jsx)(n.h3,{id:"step-6-iteration",children:"Step 6: Iteration"}),"\n",(0,r.jsx)(n.p,{children:"The process repeats for the configured number of iterations, with a delay between each round to avoid overload."}),"\n",(0,r.jsx)(n.h2,{id:"why-it-matters",children:"Why It Matters"}),"\n",(0,r.jsx)(n.p,{children:"PTZ-YOLO shifts the focus from constant video capture to selective, intelligent image collection. This is ideal for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Wildlife Monitoring"}),": Detecting animals like deer or foxes in their natural habitat"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Urban Observation"}),": Capturing activity in cities or along transit routes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Research"}),": Gathering clean, labeled images for scientific analysis"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Instead of sending endless video feeds, PTZ-YOLO captures just the high-value moments."}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,r.jsx)(n.p,{children:"PTZ-YOLO supports two model types depending on the use case:"}),"\n",(0,r.jsx)(n.h3,{id:"yolo-default",children:"YOLO (Default)"}),"\n",(0,r.jsx)(n.p,{children:'A lightweight and fast model for known categories like "person", "car", or "dog".'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'--model yolo11n --objects "person,car"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"florence",children:"Florence"}),"\n",(0,r.jsx)(n.p,{children:"A larger vision-language model with broader detection capabilities. Florence allows flexible object categories or even detection of all visible objects."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'--model Florence-base --objects "*"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["When using ",(0,r.jsx)(n.code,{children:"*"}),", the application enters a general object detection mode to inventory everything in the scene."]}),"\n",(0,r.jsx)(n.h2,{id:"where-it-runs",children:"Where It Runs"}),"\n",(0,r.jsx)(n.p,{children:"PTZ-YOLO is containerized and ready for deployment across different platforms:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPU Servers"})," (e.g. Dell blades) for high-throughput inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Waggle Nodes"})," (Jetson Xavier or AGX Orin) for sensor-rich environments in remote areas"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Simply pull the container and run with the appropriate parameters."}),"\n",(0,r.jsx)(n.h2,{id:"scientific-and-environmental-use",children:"Scientific and Environmental Use"}),"\n",(0,r.jsx)(n.p,{children:"PTZ-YOLO enables long-term, low-impact observation of ecological systems. Researchers can:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Track animal behavior without human presence"}),"\n",(0,r.jsx)(n.li,{children:"Detect species activity trends over time"}),"\n",(0,r.jsx)(n.li,{children:"Build labeled datasets for conservation or AI training"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Captured images are enriched with metadata like timestamp, location, and object labels to support later analysis."}),"\n",(0,r.jsx)(n.h2,{id:"whats-coming-next",children:"What\u2019s Coming Next"}),"\n",(0,r.jsx)(n.p,{children:"The project has several exciting directions ahead:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Species-Aware Classification"})," using more granular prompts with Florence"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event-Based Triggers"})," for audio or motion-based image capture"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Camera Coordination"})," to track objects across views"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Active Learning Integration"})," to refine models based on field data"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"get-involved",children:"Get Involved"}),"\n",(0,r.jsx)(n.p,{children:"If you are working on smart sensing, wildlife AI, or remote image collection, we invite you to explore PTZ-YOLO. It is open, modular, and ready for collaboration."}),"\n",(0,r.jsx)(n.p,{children:"Check out the code on GitHub or contact us to get started."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);